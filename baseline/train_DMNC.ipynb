{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNtvQeI4fpUcVuDSBpVJFoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jR1DbhgikTl2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712459293448,"user_tz":240,"elapsed":6353,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"0af6c674-b33f-4cba-86ca-8a5e8d7a40fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dill\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/116.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dill\n","Successfully installed dill-0.3.8\n"]}],"source":["!pip install dill"]},{"cell_type":"code","source":["!pip install dnc"],"metadata":{"id":"-zEmeURviCRo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712459357570,"user_tz":240,"elapsed":64129,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"2254ac44-8112-4589-ff78-9a29ac94f6b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dnc\n","  Downloading dnc-1.1.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from dnc) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dnc) (1.25.2)\n","Collecting flann (from dnc)\n","  Downloading flann-1.6.13-py3-none-any.whl (24 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->dnc)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->dnc)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->dnc)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->dnc)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->dnc)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->dnc)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->dnc)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->dnc)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->dnc)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->dnc)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->dnc)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->dnc) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->dnc)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->dnc) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->dnc) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, flann, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dnc\n","Successfully installed dnc-1.1.0 flann-1.6.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["# View and modify the working path\n","import os\n","from google.colab import drive\n","\n","# View current working directory\n","print(\"Current Working Directory:\", os.getcwd())\n","\n","# Mount Google Drive\n","drive.mount('/content/gdrive')\n","\n","# Change working directory to your file position\n","path = \"/content/gdrive/My Drive/GAMENet\"\n","os.chdir(path)\n","\n","# Confirm the change\n","print(\"Working Directory:\", os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwQ9SOjDkVNE","executionInfo":{"status":"ok","timestamp":1712459382525,"user_tz":240,"elapsed":25006,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"8ea2f3f8-770c-40a3-a648-28939c2aced5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory: /content\n","Mounted at /content/gdrive\n","Working Directory: /content/gdrive/My Drive/GAMENet\n"]}]},{"cell_type":"code","source":["# from google.colab import files\n","# src = list(files.upload().values())[0]\n","# open('models.py','wb').write(src)\n","# import models"],"metadata":{"id":"L8mCXKnVh5_O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import models\n","import util"],"metadata":{"id":"T44P_04OkbU8","executionInfo":{"status":"ok","timestamp":1712459393355,"user_tz":240,"elapsed":8079,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# from google.colab import files\n","# src = list(files.upload().values())[0]\n","# open('layers.py','wb').write(src)\n","# import layers"],"metadata":{"id":"nt8LKKw_jenH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wBstR2d9Cm5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n","import numpy as np\n","import dill\n","import time\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import os\n","from collections import defaultdict\n","import torch.nn.functional as F\n","\n","import sys\n","sys.path.append(\"..\")\n","from models import DMNC\n","from util import llprint, sequence_metric, ddi_rate_score, get_n_params\n","\n","torch.manual_seed(1203)\n","model_name = 'DMNC'\n","resume_name = ''\n","\n","# Enable anomaly detection\n","torch.autograd.set_detect_anomaly(True)\n","\n","'''\n","It's better to refer to the offical implement in tensorflow.  https://github.com/thaihungle/DMNC\n","'''\n","\n","def sequence_output_process(output_logits, filter_token):\n","    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n","    out_list = []\n","    for i in range(len(pind)):\n","        for j in range(pind.shape[1]):\n","            label = pind[i][j]\n","            if label in filter_token:\n","                continue\n","            if label not in out_list:\n","                out_list.append(label)\n","                break\n","    y_pred_prob_tmp = []\n","    for idx, item in enumerate(out_list):\n","        y_pred_prob_tmp.append(output_logits[idx, item])\n","    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n","    return out_list, sorted_predict\n","\n","def eval(model, data_eval, voc_size, epoch):\n","    # evaluate\n","    print('')\n","    model.eval()\n","\n","    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n","    records = []\n","    for step, input in enumerate(data_eval):\n","        y_gt = []\n","        y_pred = []\n","        y_pred_prob = []\n","        y_pred_label = []\n","        i1_state, i2_state, i3_state = None, None, None\n","        for adm in input:\n","            y_gt_tmp = np.zeros(voc_size[2])\n","            y_gt_tmp[adm[2]] = 1\n","            y_gt.append(y_gt_tmp)\n","\n","            output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n","            output_logits = output_logits.detach().cpu().numpy()\n","\n","            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n","\n","            y_pred_label.append(sorted_predict)\n","            y_pred_prob.append(np.mean(output_logits[:,:-2], axis=0))\n","\n","            y_pred_tmp = np.zeros(voc_size[2])\n","            y_pred_tmp[out_list] = 1\n","            y_pred.append(y_pred_tmp)\n","        records.append(y_pred_label)\n","\n","        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred),\n","                                                                              np.array(y_pred_prob),\n","                                                                              np.array(y_pred_label))\n","        ja.append(adm_ja)\n","        prauc.append(adm_prauc)\n","        avg_p.append(adm_avg_p)\n","        avg_r.append(adm_avg_r)\n","        avg_f1.append(adm_avg_f1)\n","\n","        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n","\n","    # ddi rate\n","    ddi_rate = ddi_rate_score(records)\n","    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n","        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","    ))\n","    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","\n","def main():\n","    if not os.path.exists(os.path.join(\"saved\", model_name)):\n","        os.makedirs(os.path.join(\"saved\", model_name))\n","\n","    data_path = './data/records_final.pkl'\n","    voc_path = './data/voc_final.pkl'\n","    device = torch.device('cuda:0')\n","\n","    data = dill.load(open(data_path, 'rb'))\n","    voc = dill.load(open(voc_path, 'rb'))\n","    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n","\n","    split_point = int(len(data) * 2 / 3)\n","    data_train = data[:split_point]\n","    eval_len = int(len(data[split_point:]) / 2)\n","    data_test = data[split_point:split_point + eval_len]\n","    data_eval = data[split_point+eval_len:]\n","    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n","\n","    EPOCH = 30\n","    LR = 0.0005\n","    TEST = False\n","    END_TOKEN = voc_size[2] + 1\n","\n","    model = DMNC(voc_size, device=device)\n","    if TEST:\n","        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n","    model.to(device=device)\n","    print('parameters', get_n_params(model))\n","\n","    criterion2 = nn.CrossEntropyLoss().to(device)\n","    optimizer = Adam(model.parameters(), lr=LR)\n","\n","    if TEST:\n","        with torch.no_grad():  # Wrap evaluation forward pass with torch.no_grad()\n","            eval(model, data_test, voc_size, 0)\n","    else:\n","        history = defaultdict(list)\n","        for epoch in range(EPOCH):\n","            loss_record1 = []\n","            loss_record2 = []\n","            start_time = time.time()\n","            model.train()\n","            # for step, input in enumerate(data_train):\n","            #     optimizer.zero_grad()  # Explicitly zero gradients before computing new gradients\n","            #     i1_state, i2_state, i3_state = None, None, None\n","            #     for adm in input:\n","            #         loss_target = adm[2] + [END_TOKEN]\n","            #         loss_target_copy = loss_target.copy()  # Create a copy to avoid modifying the original tensor\n","\n","            #         output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n","            #         loss = criterion2(output_logits, torch.LongTensor(loss_target).to(device))\n","\n","            #         loss_record1.append(loss.item())\n","            #         loss_record2.append(loss.item())\n","\n","            #         loss.backward(retain_graph=True)  # Compute gradients\n","            #         optimizer.step()  # Update model parameters\n","\n","            #     llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n","\n","            for step, input in enumerate(data_train):\n","              i1_state, i2_state, i3_state = None, None, None\n","              for adm in input:\n","                  loss_target = adm[2] + [END_TOKEN]\n","                  loss_target_copy = loss_target.copy()  # Create a copy to avoid modifying the original tensor\n","                  output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n","                  loss = criterion2(output_logits, torch.LongTensor(loss_target_copy).to(device))\n","\n","                  loss_record1.append(loss.item())\n","                  loss_record2.append(loss.item())\n","\n","                  optimizer.zero_grad()\n","                  loss.backward(retain_graph=True)\n","                  optimizer.step()\n","\n","              llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n","\n","\n","            with torch.no_grad():  # Wrap evaluation forward pass with torch.no_grad()\n","                ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n","            history['ja'].append(ja)\n","            history['ddi_rate'].append(ddi_rate)\n","            history['avg_p'].append(avg_p)\n","            history['avg_r'].append(avg_r)\n","            history['avg_f1'].append(avg_f1)\n","            history['prauc'].append(prauc)\n","\n","            end_time = time.time()\n","            elapsed_time = (end_time - start_time) / 60\n","            llprint('\\tEpoch: %d, Loss1: %.4f, Loss2: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n","                                                                                                np.mean(loss_record1),\n","                                                                                                np.mean(loss_record2),\n","                                                                                                elapsed_time,\n","                                                                                                elapsed_time * (\n","                                                                                                            EPOCH - epoch - 1)/60))\n","\n","            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n","            print('')\n","\n","        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n","\n","        # test\n","        torch.save(model.state_dict(), open(\n","            os.path.join('saved', model_name, 'final.model'), 'wb'))\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"yF7TVHWPjuJw","executionInfo":{"status":"error","timestamp":1712460326229,"user_tz":240,"elapsed":4108,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"8b05a2fa-4d94-4e1e-85a4-2f922f3d1b8c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["parameters 527979\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 136]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-0e018358c0cd>\u001b[0m in \u001b[0;36m<cell line: 202>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-0e018358c0cd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 136]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9q-rN67pjxtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xE7Ahkb0juL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n","import numpy as np\n","import dill\n","import time\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import os\n","from collections import defaultdict\n","import torch.nn.functional as F\n","\n","import sys\n","sys.path.append(\"..\")\n","from models import DMNC\n","from util import llprint, sequence_metric, ddi_rate_score, get_n_params\n","\n","torch.manual_seed(1203)\n","model_name = 'DMNC'\n","resume_name = ''\n","\n","# Enable anomaly detection\n","torch.autograd.set_detect_anomaly(True)\n","\n","'''\n","It's better to refer to the offical implement in tensorflow.  https://github.com/thaihungle/DMNC\n","'''\n","\n","def sequence_output_process(output_logits, filter_token):\n","    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n","    out_list = []\n","    for i in range(len(pind)):\n","        for j in range(pind.shape[1]):\n","            label = pind[i][j]\n","            if label in filter_token:\n","                continue\n","            if label not in out_list:\n","                out_list.append(label)\n","                break\n","    y_pred_prob_tmp = []\n","    for idx, item in enumerate(out_list):\n","        y_pred_prob_tmp.append(output_logits[idx, item])\n","    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n","    return out_list, sorted_predict\n","\n","def eval(model, data_eval, voc_size, epoch):\n","    # evaluate\n","    print('')\n","    model.eval()\n","\n","    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n","    records = []\n","    for step, input in enumerate(data_eval):\n","        y_gt = []\n","        y_pred = []\n","        y_pred_prob = []\n","        y_pred_label = []\n","        i1_state, i2_state, i3_state = None, None, None\n","        for adm in input:\n","            y_gt_tmp = np.zeros(voc_size[2])\n","            y_gt_tmp[adm[2]] = 1\n","            y_gt.append(y_gt_tmp)\n","\n","            output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n","            output_logits = output_logits.detach().cpu().numpy()\n","\n","            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n","\n","            y_pred_label.append(sorted_predict)\n","            y_pred_prob.append(np.mean(output_logits[:,:-2], axis=0))\n","\n","            y_pred_tmp = np.zeros(voc_size[2])\n","            y_pred_tmp[out_list] = 1\n","            y_pred.append(y_pred_tmp)\n","        records.append(y_pred_label)\n","\n","        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred),\n","                                                                              np.array(y_pred_prob),\n","                                                                              np.array(y_pred_label))\n","        ja.append(adm_ja)\n","        prauc.append(adm_prauc)\n","        avg_p.append(adm_avg_p)\n","        avg_r.append(adm_avg_r)\n","        avg_f1.append(adm_avg_f1)\n","\n","        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n","\n","    # ddi rate\n","    ddi_rate = ddi_rate_score(records)\n","    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n","        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","    ))\n","    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","\n","def main():\n","    if not os.path.exists(os.path.join(\"saved\", model_name)):\n","        os.makedirs(os.path.join(\"saved\", model_name))\n","\n","    data_path = './data/records_final.pkl'\n","    voc_path = './data/voc_final.pkl'\n","    device = torch.device('cuda:0')\n","\n","    data = dill.load(open(data_path, 'rb'))\n","    voc = dill.load(open(voc_path, 'rb'))\n","    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n","\n","    split_point = int(len(data) * 2 / 3)\n","    data_train = data[:split_point]\n","    eval_len = int(len(data[split_point:]) / 2)\n","    data_test = data[split_point:split_point + eval_len]\n","    data_eval = data[split_point+eval_len:]\n","    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n","\n","    EPOCH = 30\n","    LR = 0.0005\n","    TEST = False\n","    END_TOKEN = voc_size[2] + 1\n","\n","    model = DMNC(voc_size, device=device)\n","    if TEST:\n","        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n","    model.to(device=device)\n","    print('parameters', get_n_params(model))\n","\n","    criterion2 = nn.CrossEntropyLoss().to(device)\n","    optimizer = Adam(model.parameters(), lr=LR)\n","\n","    if TEST:\n","        eval(model, data_test, voc_size, 0)\n","    else:\n","        history = defaultdict(list)\n","        for epoch in range(EPOCH):\n","            loss_record1 = []\n","            loss_record2 = []\n","            start_time = time.time()\n","            model.train()\n","            for step, input in enumerate(data_train):\n","                i1_state, i2_state, i3_state = None, None, None\n","                for adm in input:\n","                    loss_target = adm[2] + [END_TOKEN]\n","                    # loss_target = adm[2].copy() + [END_TOKEN]  # Use copy to avoid modifying the original tensor\n","                    output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n","                    # loss_target_tensor = torch.LongTensor(loss_target).to(device)  # Ensure conversion to tensor is not in-place\n","                    loss = criterion2(output_logits, torch.LongTensor(loss_target).to(device))\n","\n","\n","                    loss_record1.append(loss.item())\n","                    loss_record2.append(loss.item())\n","\n","                    optimizer.zero_grad()\n","                    loss.backward(retain_graph=True)\n","                    optimizer.step()\n","\n","                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n","\n","            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n","            history['ja'].append(ja)\n","            history['ddi_rate'].append(ddi_rate)\n","            history['avg_p'].append(avg_p)\n","            history['avg_r'].append(avg_r)\n","            history['avg_f1'].append(avg_f1)\n","            history['prauc'].append(prauc)\n","\n","            end_time = time.time()\n","            elapsed_time = (end_time - start_time) / 60\n","            llprint('\\tEpoch: %d, Loss1: %.4f, Loss2: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n","                                                                                                np.mean(loss_record1),\n","                                                                                                np.mean(loss_record2),\n","                                                                                                elapsed_time,\n","                                                                                                elapsed_time * (\n","                                                                                                            EPOCH - epoch - 1)/60))\n","\n","            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n","            print('')\n","\n","        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n","\n","        # test\n","        torch.save(model.state_dict(), open(\n","            os.path.join('saved', model_name, 'final.model'), 'wb'))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"uu3GzqVDjuPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(torch.cuda.is_available())"],"metadata":{"id":"DPWrVAq0kVTA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712459414564,"user_tz":240,"elapsed":281,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"13f79ef6-94db-4499-ee8f-8f047c285e19"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))"],"metadata":{"id":"Ww4IRCXDkVV0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712459416976,"user_tz":240,"elapsed":285,"user":{"displayName":"Imran Yasin","userId":"17332317180155210417"}},"outputId":"923d13be-2a3d-4534-e0c8-96e3690fd116"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Tesla T4\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FGavI2Qh_4Pe"},"execution_count":null,"outputs":[]}]}