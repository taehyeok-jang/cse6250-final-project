{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43da9efd-cf9a-405a-87c6-03a23e36353d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_pin_memory' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [MPS, Meta, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterMPS.cpp:27248 [kernel]\nMeta: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nNestedTensorCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterNestedTensorCPU.cpp:775 [kernel]\nBackendSelect: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterBackendSelect.cpp:807 [kernel]\nPython: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nTracer: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 179\u001b[0m\n\u001b[1;32m    174\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    175\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal.model\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 117\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m TEST \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    115\u001b[0m END_TOKEN \u001b[38;5;241m=\u001b[39m voc_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 117\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDMNC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoc_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TEST:\n\u001b[1;32m    119\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name, resume_name), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[0;32m~/Documents/workspace/github/repo/cse6250-final-project/GAMENet-ref/code/baseline/../models.py:169\u001b[0m, in \u001b[0;36mDMNC.__init__\u001b[0;34m(self, vocab_size, emb_dim, device)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    166\u001b[0m     [nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m vocab_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, emb_dim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([DNC(\n\u001b[1;32m    170\u001b[0m     input_size\u001b[38;5;241m=\u001b[39memb_dim,\n\u001b[1;32m    171\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39memb_dim,\n\u001b[1;32m    172\u001b[0m     rnn_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgru\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    173\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    174\u001b[0m     num_hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    175\u001b[0m     nr_cells\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m    176\u001b[0m     cell_size\u001b[38;5;241m=\u001b[39memb_dim,\n\u001b[1;32m    177\u001b[0m     read_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    178\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m     gpu_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    180\u001b[0m     independent_linears\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGRU(emb_dim \u001b[38;5;241m+\u001b[39m emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    184\u001b[0m                       batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# input: (y, r1, r2,) hidden: (hidden1, hidden2)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface_weighting \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (emb_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# 2 read head (key, str, mode)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/workspace/github/repo/cse6250-final-project/GAMENet-ref/code/baseline/../models.py:169\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    166\u001b[0m     [nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size[i] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m vocab_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, emb_dim) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mDNC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgru\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnr_cells\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindependent_linears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mGRU(emb_dim \u001b[38;5;241m+\u001b[39m emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    184\u001b[0m                       batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# input: (y, r1, r2,) hidden: (hidden1, hidden2)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface_weighting \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(emb_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (emb_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# 2 read head (key, str, mode)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4/lib/python3.8/site-packages/dnc/dnc.py:105\u001b[0m, in \u001b[0;36mDNC.__init__\u001b[0;34m(self, input_size, hidden_size, rnn_type, num_layers, num_hidden_layers, bias, batch_first, dropout, bidirectional, nr_cells, read_heads, cell_size, nonlinearity, gpu_id, independent_linears, share_memory, debug, clip)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# only one memory shared by all layers\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_memory:\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemories\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 105\u001b[0m       \u001b[43mMemory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m          \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmem_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnr_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcell_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m          \u001b[49m\u001b[43mread_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m          \u001b[49m\u001b[43mindependent_linears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindependent_linears\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m   )\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_layer_memory_shared\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemories[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# final output layer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4/lib/python3.8/site-packages/dnc/memory.py:44\u001b[0m, in \u001b[0;36mMemory.__init__\u001b[0;34m(self, input_size, mem_size, cell_size, read_heads, gpu_id, independent_linears)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface_size \u001b[38;5;241m=\u001b[39m (w \u001b[38;5;241m*\u001b[39m r) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m w) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m r) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface_size)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mI \u001b[38;5;241m=\u001b[39m \u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4/lib/python3.8/site-packages/dnc/util.py:32\u001b[0m, in \u001b[0;36mcuda\u001b[0;34m(x, grad, gpu_id)\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m   t \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcuda(gpu_id)\n\u001b[1;32m     33\u001b[0m   t\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;241m=\u001b[39mgrad\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_pin_memory' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_pin_memory' is only available for these backends: [MPS, Meta, NestedTensorCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterMPS.cpp:27248 [kernel]\nMeta: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nNestedTensorCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterNestedTensorCPU.cpp:775 [kernel]\nBackendSelect: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/build/aten/src/ATen/RegisterBackendSelect.cpp:807 [kernel]\nPython: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMTIA: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nTracer: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025541661/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import dill\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import DMNC\n",
    "from util import llprint, sequence_metric, ddi_rate_score, get_n_params\n",
    "\n",
    "torch.manual_seed(1203)\n",
    "model_name = 'DMNC'\n",
    "resume_name = ''\n",
    "\n",
    "'''\n",
    "It's better to refer to the offical implement in tensorflow.  https://github.com/thaihungle/DMNC\n",
    "'''\n",
    "\n",
    "def sequence_output_process(output_logits, filter_token):\n",
    "    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n",
    "    out_list = []\n",
    "    for i in range(len(pind)):\n",
    "        for j in range(pind.shape[1]):\n",
    "            label = pind[i][j]\n",
    "            if label in filter_token:\n",
    "                continue\n",
    "            if label not in out_list:\n",
    "                out_list.append(label)\n",
    "                break\n",
    "    y_pred_prob_tmp = []\n",
    "    for idx, item in enumerate(out_list):\n",
    "        y_pred_prob_tmp.append(output_logits[idx, item])\n",
    "    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n",
    "    return out_list, sorted_predict\n",
    "\n",
    "def eval(model, data_eval, voc_size, epoch):\n",
    "    # evaluate\n",
    "    print('')\n",
    "    model.eval()\n",
    "\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
    "    records = []\n",
    "    for step, input in enumerate(data_eval):\n",
    "        y_gt = []\n",
    "        y_pred = []\n",
    "        y_pred_prob = []\n",
    "        y_pred_label = []\n",
    "        i1_state, i2_state, i3_state = None, None, None\n",
    "        for adm in input:\n",
    "            y_gt_tmp = np.zeros(voc_size[2])\n",
    "            y_gt_tmp[adm[2]] = 1\n",
    "            y_gt.append(y_gt_tmp)\n",
    "\n",
    "            output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n",
    "            output_logits = output_logits.detach().cpu().numpy()\n",
    "\n",
    "            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n",
    "\n",
    "            y_pred_label.append(sorted_predict)\n",
    "            y_pred_prob.append(np.mean(output_logits[:,:-2], axis=0))\n",
    "\n",
    "            y_pred_tmp = np.zeros(voc_size[2])\n",
    "            y_pred_tmp[out_list] = 1\n",
    "            y_pred.append(y_pred_tmp)\n",
    "        records.append(y_pred_label)\n",
    "\n",
    "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred),\n",
    "                                                                              np.array(y_pred_prob),\n",
    "                                                                              np.array(y_pred_label))\n",
    "        ja.append(adm_ja)\n",
    "        prauc.append(adm_prauc)\n",
    "        avg_p.append(adm_avg_p)\n",
    "        avg_r.append(adm_avg_r)\n",
    "        avg_f1.append(adm_avg_f1)\n",
    "\n",
    "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
    "\n",
    "    # ddi rate\n",
    "    ddi_rate = ddi_rate_score(records)\n",
    "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "    ))\n",
    "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
    "        os.makedirs(os.path.join(\"saved\", model_name))\n",
    "\n",
    "    data_path = '../../data/records_final.pkl'\n",
    "    voc_path = '../../data/voc_final.pkl'\n",
    "    # device = torch.device('cuda:0')\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
    "\n",
    "    EPOCH = 30\n",
    "    LR = 0.0005\n",
    "    TEST = False\n",
    "    END_TOKEN = voc_size[2] + 1\n",
    "\n",
    "    model = DMNC(voc_size, device=device)\n",
    "    if TEST:\n",
    "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
    "    model.to(device=device)\n",
    "    print('parameters', get_n_params(model))\n",
    "\n",
    "    criterion2 = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if TEST:\n",
    "        eval(model, data_test, voc_size, 0)\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        for epoch in range(EPOCH):\n",
    "            loss_record1 = []\n",
    "            loss_record2 = []\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            for step, input in enumerate(data_train):\n",
    "                i1_state, i2_state, i3_state = None, None, None\n",
    "                for adm in input:\n",
    "                    loss_target = adm[2] + [END_TOKEN]\n",
    "                    output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n",
    "                    loss = criterion2(output_logits, torch.LongTensor(loss_target).to(device))\n",
    "\n",
    "                    loss_record1.append(loss.item())\n",
    "                    loss_record2.append(loss.item())\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
    "\n",
    "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
    "            history['ja'].append(ja)\n",
    "            history['ddi_rate'].append(ddi_rate)\n",
    "            history['avg_p'].append(avg_p)\n",
    "            history['avg_r'].append(avg_r)\n",
    "            history['avg_f1'].append(avg_f1)\n",
    "            history['prauc'].append(prauc)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time) / 60\n",
    "            llprint('\\tEpoch: %d, Loss1: %.4f, Loss2: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
    "                                                                                                np.mean(loss_record1),\n",
    "                                                                                                np.mean(loss_record2),\n",
    "                                                                                                elapsed_time,\n",
    "                                                                                                elapsed_time * (\n",
    "                                                                                                            EPOCH - epoch - 1)/60))\n",
    "\n",
    "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
    "            print('')\n",
    "\n",
    "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "\n",
    "        # test\n",
    "        torch.save(model.state_dict(), open(\n",
    "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806963a-1522-4f51-bbc7-87287447192e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
